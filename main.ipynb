{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyenphuan/Documents/Github/AI-Summary-Paper-Tool/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"paper_summaries\")\n",
    "\n",
    "def get_closest_document(query):\n",
    "    # Step 1: Embed the query\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=query,\n",
    "    )\n",
    "    \n",
    "    query_embedding = result['embedding']\n",
    "    \n",
    "    results = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "    embeddings = results[\"embeddings\"]\n",
    "    documents = results[\"documents\"]\n",
    "    metadatas = results[\"metadatas\"]\n",
    "    \n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        return {\"error\": \"No documents found in the database.\"}\n",
    "    \n",
    "    similarities = []\n",
    "    for embedding in embeddings:\n",
    "        embedding = np.array(embedding) if isinstance(embedding, list) else embedding\n",
    "        similarity = 1 - cosine(query_embedding, embedding) # Compute cosine similarity (1 - cosine distance)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    closest_index = np.argmax(similarities)  # Index of the highest similarity score\n",
    "    closest_similarity = similarities[closest_index]\n",
    "    \n",
    "    # Step 5: Extract the closest document's details\n",
    "    closest_document = {\n",
    "        \"title\": metadatas[closest_index].get(\"title\", \"No title available\"),\n",
    "        \"content\": documents[closest_index],\n",
    "        \"similarity_score\": closest_similarity\n",
    "    }\n",
    "    \n",
    "    return closest_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_display_result(closest_doc):\n",
    "    if \"error\" in closest_doc:\n",
    "        print(closest_doc[\"error\"])\n",
    "        return\n",
    "    \n",
    "    title = closest_doc.get(\"title\", \"No title available\")\n",
    "    content = closest_doc.get(\"content\", \"No content available\")\n",
    "    similarity_score = closest_doc.get(\"similarity_score\", 0.0)\n",
    "    \n",
    "    # Print formatted output\n",
    "    print(\"\\nClosest Document:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Title: {title}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Similarity Score: {similarity_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Content:\")\n",
    "    print(content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Closest Document:\n",
      "==================================================\n",
      "Title: Attention Is All You Need\n",
      "--------------------------------------------------\n",
      "Similarity Score: 0.6909\n",
      "--------------------------------------------------\n",
      "Content:\n",
      "## Summary of \"Attention Is All You Need\"\n",
      "\n",
      "This research paper introduces the Transformer, a novel neural network architecture designed for sequence transduction tasks, such as machine translation. Unlike previous dominant models that rely on complex recurrent or convolutional neural networks, the Transformer is based solely on attention mechanisms, eliminating the need for recurrence and convolutions. The authors demonstrate that the Transformer achieves superior performance in machine translation tasks while offering greater parallelization and requiring significantly less training time.\n",
      "\n",
      "**1. Research Problem:**\n",
      "\n",
      "*   **Main Objective:** The primary objective of this study is to develop a new neural network architecture for sequence transduction that overcomes the limitations of recurrent and convolutional models, particularly in terms of parallelization and training efficiency, while maintaining or improving translation quality.\n",
      "*   **Problem Addressed:** The paper addresses the inherent sequential computation limitation of recurrent neural networks (RNNs) and the difficulty of convolutional neural networks (CNNs) in learning long-range dependencies in sequences. \"Recurrent models typically factor computation along the symbol positions of the input and output sequences... This inherently sequential nature precludes parallelization within training examples...\" Furthermore, the paper tackles the need for more efficient and effective models for sequence transduction tasks like machine translation.\n",
      "\n",
      "**2. Hypothesis/Research Questions:**\n",
      "\n",
      "*   **Main Hypothesis:** The central hypothesis is that a network architecture based solely on attention mechanisms can achieve state-of-the-art performance in sequence transduction tasks, surpassing the performance of recurrent and convolutional models, while also enabling greater parallelization and reducing training time.\n",
      "*   **Research Questions:** Can an architecture relying entirely on attention mechanisms effectively model long-range dependencies in sequences? Can this architecture be trained more efficiently than recurrent and convolutional models? Does this new architecture generalize well to different sequence transduction tasks?\n",
      "\n",
      "**3. Research Methods:**\n",
      "\n",
      "*   **Approach:** The authors propose a novel architecture called the Transformer, which replaces recurrent and convolutional layers with multi-headed self-attention mechanisms. The Transformer follows an encoder-decoder structure.\n",
      "*   **Specific Tools, Data, and Procedures:**\n",
      "    *   **Architecture:** The Transformer consists of stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder and decoder are composed of N=6 identical layers. Each layer employs residual connections and layer normalization.\n",
      "    *   **Attention Mechanism:** The core of the Transformer is the \"Scaled Dot-Product Attention,\" where the dot products of queries and keys are computed, scaled by the square root of the key dimension, and then passed through a softmax function to obtain weights for the values.\n",
      "    *   **Multi-Head Attention:** To allow the model to attend to information from different representation subspaces, the authors introduce \"Multi-Head Attention,\" where the queries, keys, and values are linearly projected h times with different learned linear projections. \"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\"\n",
      "    *   **Positional Encoding:** Since the model lacks recurrence or convolution, positional encodings are added to the input embeddings to provide information about the order of the sequence.\n",
      "    *   **Datasets:** The models were trained and evaluated on the WMT 2014 English-German and English-French machine translation datasets. They also tested the model's generalizability on English constituency parsing using the Wall Street Journal (WSJ) portion of the Penn Treebank.\n",
      "    *   **Training:** The models were trained using the Adam optimizer with a specific learning rate schedule. Regularization techniques, including residual dropout and label smoothing, were employed to prevent overfitting.\n",
      "\n",
      "**4. Results:**\n",
      "\n",
      "*   **Important Results:** The Transformer achieved state-of-the-art results on both the WMT 2014 English-German and English-French translation tasks. The \"Transformer (big)\" model achieved a BLEU score of 28.4 on the English-to-German task, surpassing previous best results by over 2 BLEU. On the English-to-French task, it achieved a BLEU score of 41.8.\n",
      "*   **Specific Data:** Table 2 in the paper summarizes the BLEU scores and training costs of the Transformer compared to other models. Table 3 shows the results of varying different components of the Transformer architecture on the English-to-German translation task. Table 4 presents the results on English constituency parsing.\n",
      "\n",
      "**5. Discussion & Analysis:**\n",
      "\n",
      "*   **Broader Context:** The results demonstrate that attention mechanisms alone can effectively model long-range dependencies in sequences, surpassing the performance of recurrent and convolutional models. The Transformer's parallelizable nature allows for significantly faster training times.\n",
      "*   **Important Conclusions:** The authors conclude that the Transformer architecture represents a significant advancement in sequence transduction modeling. The self-attention mechanism is key to its performance, allowing the model to capture relationships between different parts of the input sequence effectively. The model's ability to generalize to other tasks, such as English constituency parsing, highlights its versatility.\n",
      "\n",
      "**6. Conclusion & Recommendations:**\n",
      "\n",
      "*   **Main Conclusions:** The Transformer is a novel and effective architecture for sequence transduction, offering superior performance, greater parallelization, and reduced training time compared to recurrent and convolutional models.\n",
      "*   **Recommendations:** The authors recommend exploring the application of attention-based models to other tasks, including those involving input and output modalities other than text. They also suggest investigating local, restricted attention mechanisms for handling large inputs and outputs, such as images, audio, and video. Further research into making generation less sequential is also recommended.\n",
      "\n",
      "**7. Limitations:**\n",
      "\n",
      "*   **Limitations and Unresolved Issues:** The paper acknowledges that while the Transformer achieves state-of-the-art results, there is room for improvement. The authors suggest further research into local, restricted attention mechanisms for handling large inputs and outputs, which could address the computational complexity associated with self-attention when dealing with very long sequences. Making the generation process less sequential is another area for future research.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to learn about attention transformer in deep learning\"\n",
    "result = get_closest_document(query)\n",
    "\n",
    "format_and_display_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\AI-Summary-Paper-Tool\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"paper_summaries\")\n",
    "\n",
    "def get_closest_document(query):\n",
    "    # Step 1: Embed the query\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=query,\n",
    "    )\n",
    "    \n",
    "    query_embedding = result['embedding']\n",
    "    \n",
    "    results = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "    embeddings = results[\"embeddings\"]\n",
    "    documents = results[\"documents\"]\n",
    "    metadatas = results[\"metadatas\"]\n",
    "    \n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        return {\"error\": \"No documents found in the database.\"}\n",
    "    \n",
    "    similarities = []\n",
    "    for embedding in embeddings:\n",
    "        embedding = np.array(embedding) if isinstance(embedding, list) else embedding\n",
    "        similarity = 1 - cosine(query_embedding, embedding) # Compute cosine similarity (1 - cosine distance)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    closest_index = np.argmax(similarities)  # Index of the highest similarity score\n",
    "    closest_similarity = similarities[closest_index]\n",
    "    \n",
    "    # Step 5: Extract the closest document's details\n",
    "    closest_document = {\n",
    "        \"title\": metadatas[closest_index].get(\"title\", \"No title available\"),\n",
    "        \"content\": documents[closest_index],\n",
    "        \"similarity_score\": closest_similarity\n",
    "    }\n",
    "    \n",
    "    return closest_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_display_result(closest_doc):\n",
    "    if \"error\" in closest_doc:\n",
    "        print(closest_doc[\"error\"])\n",
    "        return\n",
    "    \n",
    "    title = closest_doc.get(\"title\", \"No title available\")\n",
    "    content = closest_doc.get(\"content\", \"No content available\")\n",
    "    similarity_score = closest_doc.get(\"similarity_score\", 0.0)\n",
    "    \n",
    "    # Print formatted output\n",
    "    print(\"\\nClosest Document:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Title: {title}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Similarity Score: {similarity_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Content:\")\n",
    "    print(content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Closest Document:\n",
      "==================================================\n",
      "Title: Differentially Private Attention Computation\n",
      "--------------------------------------------------\n",
      "Similarity Score: 0.6847\n",
      "--------------------------------------------------\n",
      "Content:\n",
      "Here is a summary of the research paper, addressing your specific questions and requirements:\n",
      "\n",
      "**Summary of \"Differentially Private Attention Computation\"**\n",
      "\n",
      "This research paper tackles the critical problem of privacy in large language models (LLMs), specifically concerning the attention mechanism, a core component of Transformer architectures. The authors identify that LLMs can inadvertently leak sensitive information present in training data or user inputs during inference. To mitigate this risk, the paper proposes a novel and efficient algorithm for approximating the attention matrix while providing provable differential privacy (DP) guarantees.\n",
      "\n",
      "**1. Research Problem:**\n",
      "\n",
      "*   **Main Objective:** The primary objective is to develop an algorithm that computes the attention matrix in LLMs while ensuring differential privacy, preventing the leakage of sensitive information.\n",
      "\n",
      "*   **Problem/Question:** The paper aims to solve the problem of privacy risks associated with the attention mechanism in LLMs, which can expose confidential or copyrighted information. It addresses the question of how to compute the attention matrix with provable privacy guarantees, specifically focusing on static attention computation.\n",
      "\n",
      "**2. Hypothesis/Research Questions:**\n",
      "\n",
      "The paper does not explicitly state a hypothesis in the traditional sense. Instead, it focuses on demonstrating the feasibility and effectiveness of a differentially private attention computation algorithm. The central research question is:\n",
      "\n",
      "*   Can an efficient algorithm be designed to approximate the attention matrix with provable differential privacy guarantees, balancing privacy protection with computational efficiency and accuracy?\n",
      "\n",
      "**3. Research Methods:**\n",
      "\n",
      "*   **Approach:** The authors employ a theoretical approach, building upon existing research in fast attention computation and differentially private matrix publishing. The approach involves developing a novel algorithm for approximating the attention matrix and rigorously analyzing its privacy and utility properties. They focus on static attention computation.\n",
      "\n",
      "*   **Tools, Data, and Procedures:**\n",
      "\n",
      "    *   **Theoretical Framework:** Differential privacy (specifically (ε, δ)-differential privacy) is used as the privacy standard.\n",
      "    *   **Algorithms:** The core of the paper is the development of a novel algorithm (Algorithm 1 and Algorithm 2) for differentially private attention computation. It uses a differentially private covariance releasing mechanism as a subroutine.\n",
      "    *   **Mathematical Analysis:** The paper relies heavily on mathematical analysis to prove the privacy guarantees and utility bounds of the proposed algorithm. This includes:\n",
      "        *   Error analysis of matrix perturbation.\n",
      "        *   Sensitivity analysis of the attention matrix computation with respect to changes in the input data.\n",
      "        *   Analysis of the Gaussian sampling mechanism used for differential privacy.\n",
      "    *   **Definitions:** Key definitions are provided for attention computation, differential privacy, neighboring datasets, and \"good\" datasets.\n",
      "\n",
      "**4. Results:**\n",
      "\n",
      "*   **Important Results:** The main result is the development and analysis of an algorithm that achieves (ε, δ)-differential privacy while approximating the attention matrix. The paper demonstrates that the proposed algorithm can compute a private approximation of the attention mechanism for functions such as f(z) = exp(z) or f(z) = cosh(z).\n",
      "\n",
      "*   **Specific Data/Information:** Theorem 4.1 (Main result) formally states the privacy and utility guarantees of the algorithm. It provides a bound on the difference between the normalized attention matrices derived from the original data and the differentially private approximation. The theorem states that:\n",
      "    *   There exists an algorithm that takes X as input and produces a matrix B and also attention D(B) 1f(B) as output such that\n",
      "    *   D(A) 1f(A) D(B) 1f(B) 4 (1+ǫ+2r) r.\n",
      "    *   With respect to X, the algorithm is (ǫ,δ)-differential private.\n",
      "    *   It holds with probability 1 γ.\n",
      "\n",
      "**5. Discussion & Analysis:**\n",
      "\n",
      "*   **Broader Context:** The results contribute to the growing body of research on privacy-preserving machine learning, specifically in the context of LLMs. The paper highlights the importance of addressing privacy risks in attention mechanisms, which are fundamental to the performance of modern LLMs.\n",
      "\n",
      "*   **Important Conclusions:** The authors conclude that it is possible to implement attention mechanisms in a way that maintains both model performance (utility) and data privacy. The developed algorithm provides a practical approach for protecting sensitive information in LLMs.\n",
      "\n",
      "**6. Conclusion & Recommendations:**\n",
      "\n",
      "*   **Main Conclusions:** The paper concludes that the proposed algorithm provides a viable solution for differentially private attention computation, balancing privacy protection with accuracy and efficiency.\n",
      "\n",
      "*   **Recommendations:** The authors suggest that their work can serve as a starting point for developing fully DP training and inference algorithms for LLMs. They also identify approximating asymmetric attention computation with differential privacy as an interesting open problem for future research.\n",
      "\n",
      "**7. Limitations:**\n",
      "\n",
      "*   The algorithm's guarantees rely on certain properties of the input data, as defined by the \"(α, η)-good\" dataset condition. This might limit its applicability to certain types of data.\n",
      "*   The paper focuses on static attention computation, which may not be suitable for all LLM applications.\n",
      "*   The utility bounds provided in the paper may not be tight, and further research could explore ways to improve the accuracy of the approximation while maintaining differential privacy.\n",
      "*   The experimental validation of the proposed algorithm is missing.\n",
      "\n",
      "**Cited Sentences:**\n",
      "\n",
      "*   \"In this work, we propose a novel and efficient algorithm for approximating the attention matrix while providing differential privacy (DP) guarantees.\"\n",
      "*   \"Given that attention mechanisms are at the core of models like the Transformer [VSP+17], considering privacy in attention computation is crucial.\"\n",
      "*   \"Our results rely on good properties of the input data, which are defined as follows. They play a crucial role in the analysis of sensitivity with respect to (X) = XX (See Section 5.3).\"\n",
      "*   \"To the best of our knowledge, this is the first work of accelerating attention computation in the DP setting.\"\n",
      "\n",
      "This summary provides a comprehensive overview of the research paper, addressing the key aspects and providing relevant details as requested.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to learn about differentially private machine learning algorithms.\"\n",
    "result = get_closest_document(query)\n",
    "\n",
    "format_and_display_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
